{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LoRA Recipe Tutorial\n",
    "\n",
    "This tutorial teaches you how to create custom training recipes in SparkTrainer. You'll learn:\n",
    "\n",
    "1. Understanding the Recipe System\n",
    "2. Creating a Custom LoRA Recipe\n",
    "3. Configuring Advanced LoRA Parameters\n",
    "4. Implementing Custom Training Logic\n",
    "5. Registering and Using Your Recipe\n",
    "\n",
    "## What is a Training Recipe?\n",
    "\n",
    "A **training recipe** in SparkTrainer is a reusable configuration that defines:\n",
    "- How to prepare data\n",
    "- How to build the model\n",
    "- How to train the model\n",
    "- How to evaluate the model\n",
    "\n",
    "This abstraction makes it easy to experiment with different training strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers peft bitsandbytes datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recipe Interface\n",
    "\n",
    "All recipes inherit from the `TrainerRecipe` base class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/app/src')  # Adjust path as needed\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data loading\"\"\"\n",
    "    dataset_path: str\n",
    "    batch_size: int = 4\n",
    "    max_length: int = 2048\n",
    "    train_split: float = 0.9\n",
    "    val_split: float = 0.05\n",
    "    test_split: float = 0.05\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model setup\"\"\"\n",
    "    model_name: str\n",
    "    quantization: Optional[str] = None  # '4bit', '8bit', None\n",
    "    device_map: str = 'auto'\n",
    "    torch_dtype: str = 'float16'\n",
    "\n",
    "@dataclass\n",
    "class LoRAConfig:\n",
    "    \"\"\"LoRA-specific configuration\"\"\"\n",
    "    r: int = 16  # Rank of LoRA matrices\n",
    "    lora_alpha: int = 32  # Scaling factor\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: list = field(default_factory=lambda: ['q_proj', 'v_proj'])\n",
    "    bias: str = 'none'\n",
    "    task_type: str = 'CAUSAL_LM'\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for training\"\"\"\n",
    "    learning_rate: float = 2e-4\n",
    "    num_epochs: int = 3\n",
    "    warmup_steps: int = 100\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    max_grad_norm: float = 1.0\n",
    "    optimizer: str = 'adamw_torch'\n",
    "    lr_scheduler: str = 'cosine'\n",
    "\n",
    "print(\"Recipe configuration classes loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Custom LoRA Recipe\n",
    "\n",
    "Let's create a custom recipe that implements **LoRA with 4-bit quantization** (QLoRA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomQLoRARecipe:\n",
    "    \"\"\"\n",
    "    Custom QLoRA Recipe for efficient fine-tuning.\n",
    "    \n",
    "    This recipe implements:\n",
    "    - 4-bit quantization for memory efficiency\n",
    "    - LoRA adapters for parameter-efficient training\n",
    "    - Gradient checkpointing for reduced memory\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: ModelConfig,\n",
    "        lora_config: LoRAConfig,\n",
    "        training_config: TrainingConfig\n",
    "    ):\n",
    "        self.model_config = model_config\n",
    "        self.lora_config = lora_config\n",
    "        self.training_config = training_config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "    \n",
    "    def prepare_quantization_config(self) -> BitsAndBytesConfig:\n",
    "        \"\"\"\n",
    "        Configure 4-bit quantization using bitsandbytes.\n",
    "        \n",
    "        Returns:\n",
    "            BitsAndBytesConfig for 4-bit NormalFloat quantization\n",
    "        \"\"\"\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,  # Nested quantization\n",
    "            bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 quantization\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,  # Compute dtype\n",
    "        )\n",
    "    \n",
    "    def build_model(self) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Build the quantized model with LoRA adapters.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (model, tokenizer)\n",
    "        \"\"\"\n",
    "        print(f\"Loading model: {self.model_config.model_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_config.model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Set pad token if not present\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Configure quantization\n",
    "        quantization_config = self.prepare_quantization_config()\n",
    "        \n",
    "        # Load base model with quantization\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_config.model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=self.model_config.device_map,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        print(f\"Base model loaded. Parameters: {base_model.num_parameters():,}\")\n",
    "        \n",
    "        # Prepare model for k-bit training\n",
    "        base_model = prepare_model_for_kbit_training(\n",
    "            base_model,\n",
    "            use_gradient_checkpointing=True\n",
    "        )\n",
    "        \n",
    "        # Configure LoRA\n",
    "        peft_config = LoraConfig(\n",
    "            r=self.lora_config.r,\n",
    "            lora_alpha=self.lora_config.lora_alpha,\n",
    "            lora_dropout=self.lora_config.lora_dropout,\n",
    "            target_modules=self.lora_config.target_modules,\n",
    "            bias=self.lora_config.bias,\n",
    "            task_type=self.lora_config.task_type,\n",
    "        )\n",
    "        \n",
    "        # Apply LoRA\n",
    "        self.model = get_peft_model(base_model, peft_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        all_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_percent = 100 * trainable_params / all_params\n",
    "        \n",
    "        print(f\"\\nLoRA Configuration:\")\n",
    "        print(f\"  Rank (r): {self.lora_config.r}\")\n",
    "        print(f\"  Alpha: {self.lora_config.lora_alpha}\")\n",
    "        print(f\"  Dropout: {self.lora_config.lora_dropout}\")\n",
    "        print(f\"  Target modules: {self.lora_config.target_modules}\")\n",
    "        print(f\"\\nTrainable Parameters:\")\n",
    "        print(f\"  Total: {all_params:,}\")\n",
    "        print(f\"  Trainable: {trainable_params:,} ({trainable_percent:.2f}%)\")\n",
    "        \n",
    "        return self.model, self.tokenizer\n",
    "    \n",
    "    def prepare_data(self, data_config: DataConfig):\n",
    "        \"\"\"\n",
    "        Prepare and tokenize training data.\n",
    "        \n",
    "        Args:\n",
    "            data_config: Configuration for data loading\n",
    "        \"\"\"\n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        print(f\"Loading dataset: {data_config.dataset_path}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = load_dataset(data_config.dataset_path)\n",
    "        \n",
    "        # Tokenization function\n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples['text'],\n",
    "                truncation=True,\n",
    "                max_length=data_config.max_length,\n",
    "                padding='max_length'\n",
    "            )\n",
    "        \n",
    "        # Tokenize dataset\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset['train'].column_names\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset prepared. Train size: {len(tokenized_dataset['train'])}\")\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    def train(self, dataset, output_dir: str = './outputs'):\n",
    "        \"\"\"\n",
    "        Train the model using the Hugging Face Trainer.\n",
    "        \n",
    "        Args:\n",
    "            dataset: Tokenized dataset\n",
    "            output_dir: Directory to save checkpoints\n",
    "        \"\"\"\n",
    "        from transformers import Trainer, TrainingArguments\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=self.training_config.num_epochs,\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=self.training_config.gradient_accumulation_steps,\n",
    "            warmup_steps=self.training_config.warmup_steps,\n",
    "            learning_rate=self.training_config.learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            save_strategy='epoch',\n",
    "            evaluation_strategy='epoch',\n",
    "            max_grad_norm=self.training_config.max_grad_norm,\n",
    "            optim=self.training_config.optimizer,\n",
    "            lr_scheduler_type=self.training_config.lr_scheduler,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset['train'],\n",
    "            eval_dataset=dataset.get('validation', dataset.get('test')),\n",
    "        )\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        trainer.save_model(f\"{output_dir}/final\")\n",
    "        \n",
    "        print(f\"Training complete! Model saved to {output_dir}/final\")\n",
    "        \n",
    "        return trainer\n",
    "\n",
    "print(\"Custom QLoRA Recipe defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure and Run the Recipe\n",
    "\n",
    "Now let's configure and run our custom recipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "model_config = ModelConfig(\n",
    "    model_name='meta-llama/Llama-2-7b-hf',  # Replace with your model\n",
    "    quantization='4bit',\n",
    "    device_map='auto',\n",
    "    torch_dtype='bfloat16'\n",
    ")\n",
    "\n",
    "# Configure LoRA with custom parameters\n",
    "lora_config = LoRAConfig(\n",
    "    r=32,  # Higher rank for more capacity\n",
    "    lora_alpha=64,  # 2x rank is typical\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],  # Apply to all attention\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "# Configure training\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=2e-4,\n",
    "    num_epochs=3,\n",
    "    warmup_steps=100,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_grad_norm=1.0,\n",
    "    optimizer='adamw_torch',\n",
    "    lr_scheduler='cosine'\n",
    ")\n",
    "\n",
    "# Create recipe instance\n",
    "recipe = CustomQLoRARecipe(\n",
    "    model_config=model_config,\n",
    "    lora_config=lora_config,\n",
    "    training_config=training_config\n",
    ")\n",
    "\n",
    "print(\"Recipe configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model, tokenizer = recipe.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data_config = DataConfig(\n",
    "    dataset_path='your-dataset',  # Replace with your dataset\n",
    "    batch_size=4,\n",
    "    max_length=2048\n",
    ")\n",
    "\n",
    "dataset = recipe.prepare_data(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer = recipe.train(dataset, output_dir='./custom_lora_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced LoRA Techniques\n",
    "\n",
    "Let's explore some advanced LoRA configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 1: Multi-Head LoRA (Different ranks for different modules)\n",
    "advanced_lora_config = {\n",
    "    'q_proj': {'r': 32, 'lora_alpha': 64},\n",
    "    'v_proj': {'r': 32, 'lora_alpha': 64},\n",
    "    'k_proj': {'r': 16, 'lora_alpha': 32},  # Lower rank for keys\n",
    "    'o_proj': {'r': 16, 'lora_alpha': 32},  # Lower rank for output\n",
    "}\n",
    "\n",
    "print(\"Advanced LoRA Configuration:\")\n",
    "print(advanced_lora_config)\n",
    "\n",
    "# Technique 2: Dynamic Rank Selection\n",
    "def suggest_lora_rank(model_size_gb: float) -> int:\n",
    "    \"\"\"\n",
    "    Suggest optimal LoRA rank based on model size.\n",
    "    \n",
    "    Args:\n",
    "        model_size_gb: Model size in GB\n",
    "    \n",
    "    Returns:\n",
    "        Suggested rank\n",
    "    \"\"\"\n",
    "    if model_size_gb < 3:  # Small models (< 3B params)\n",
    "        return 8\n",
    "    elif model_size_gb < 15:  # Medium models (3-13B params)\n",
    "        return 16\n",
    "    elif model_size_gb < 30:  # Large models (13-30B params)\n",
    "        return 32\n",
    "    else:  # Very large models (> 30B params)\n",
    "        return 64\n",
    "\n",
    "# Example usage\n",
    "model_size = 14  # GB (approximate for 7B model)\n",
    "suggested_rank = suggest_lora_rank(model_size)\n",
    "print(f\"\\nFor a {model_size}GB model, suggested LoRA rank: {suggested_rank}\")\n",
    "\n",
    "# Technique 3: LoRA with Layerwise Learning Rates\n",
    "def create_layerwise_lr_config(base_lr: float, num_layers: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Create layerwise learning rate configuration.\n",
    "    Lower layers get smaller LR, higher layers get larger LR.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    for i in range(num_layers):\n",
    "        # Linear scaling from 0.5x to 1.5x base LR\n",
    "        layer_lr = base_lr * (0.5 + (i / num_layers))\n",
    "        config[f'layer_{i}'] = layer_lr\n",
    "    return config\n",
    "\n",
    "layerwise_config = create_layerwise_lr_config(base_lr=2e-4, num_layers=32)\n",
    "print(f\"\\nLayerwise LR (first 5 layers): {dict(list(layerwise_config.items())[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Register Recipe with SparkTrainer\n",
    "\n",
    "To use your custom recipe in SparkTrainer, register it via the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# SparkTrainer API\n",
    "API_BASE = \"http://localhost:5000/api\"\n",
    "\n",
    "# Recipe registration payload\n",
    "recipe_payload = {\n",
    "    \"name\": \"custom_qlora_v1\",\n",
    "    \"type\": \"adapter\",\n",
    "    \"description\": \"Custom QLoRA recipe with advanced configurations\",\n",
    "    \"compatible_models\": [\"llama\", \"mistral\", \"gpt\"],\n",
    "    \"hyperparameters\": {\n",
    "        \"lora_r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"num_epochs\": 3,\n",
    "        \"quantization\": \"4bit\"\n",
    "    },\n",
    "    \"code_path\": \"/path/to/custom_qlora_recipe.py\",\n",
    "    \"version\": \"1.0.0\"\n",
    "}\n",
    "\n",
    "# Register the recipe\n",
    "response = requests.post(\n",
    "    f\"{API_BASE}/recipes\",\n",
    "    json=recipe_payload\n",
    ")\n",
    "\n",
    "if response.status_code == 201:\n",
    "    print(\"‚úÖ Recipe registered successfully!\")\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "else:\n",
    "    print(f\"‚ùå Registration failed: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use Your Custom Recipe in a Job\n",
    "\n",
    "Now you can use your custom recipe in training jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a job using your custom recipe\n",
    "job_config = {\n",
    "    \"name\": \"custom-qlora-training\",\n",
    "    \"recipe\": \"custom_qlora_v1\",  # Your custom recipe\n",
    "    \"base_model\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"dataset\": \"my-dataset\",\n",
    "    \"hyperparameters\": {\n",
    "        \"lora_r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"learning_rate\": 2e-4\n",
    "    },\n",
    "    \"resources\": {\n",
    "        \"gpu_count\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{API_BASE}/jobs\", json=job_config)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    job = response.json()\n",
    "    print(f\"‚úÖ Job created with custom recipe!\")\n",
    "    print(f\"Job ID: {job['id']}\")\n",
    "    print(f\"Recipe: {job.get('recipe', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"‚ùå Job creation failed: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recipe Best Practices\n",
    "\n",
    "Here are some best practices for creating effective training recipes:\n",
    "\n",
    "### Memory Optimization\n",
    "\n",
    "1. **Use Gradient Checkpointing**: Trades compute for memory\n",
    "2. **Enable Gradient Accumulation**: Effective batch size without memory overhead\n",
    "3. **Mixed Precision Training**: Use FP16 or BF16\n",
    "4. **Quantization**: 4-bit or 8-bit for large models\n",
    "\n",
    "### LoRA Configuration\n",
    "\n",
    "1. **Start with r=8-16**: Increase only if underfitting\n",
    "2. **Alpha = 2 * r**: Good starting point\n",
    "3. **Target Critical Modules**: Focus on attention (Q, K, V)\n",
    "4. **Use Dropout**: Prevents overfitting (0.05-0.1)\n",
    "\n",
    "### Training Stability\n",
    "\n",
    "1. **Warmup**: Use 5-10% of total steps\n",
    "2. **Learning Rate**: 2e-4 to 2e-5 for LoRA\n",
    "3. **Gradient Clipping**: Prevent exploding gradients (1.0)\n",
    "4. **Cosine Schedule**: Smooth LR decay\n",
    "\n",
    "### Monitoring\n",
    "\n",
    "1. **Log Frequently**: Every 10-50 steps\n",
    "2. **Track Loss**: Both training and validation\n",
    "3. **Monitor Gradients**: Check for vanishing/exploding\n",
    "4. **Evaluate Regularly**: Every epoch or half-epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned how to:\n",
    "\n",
    "- ‚úÖ Understand the SparkTrainer recipe system\n",
    "- ‚úÖ Create custom LoRA recipes\n",
    "- ‚úÖ Configure advanced LoRA parameters\n",
    "- ‚úÖ Implement custom training logic\n",
    "- ‚úÖ Register and use recipes in SparkTrainer\n",
    "- ‚úÖ Apply best practices for recipe development\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Multimodal Training** (`03_multimodal_training.ipynb`)\n",
    "- **Hyperparameter Optimization** (`04_advanced_optimization.ipynb`)\n",
    "- **Model Deployment** (`05_model_deployment.ipynb`)\n",
    "\n",
    "Happy training! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
