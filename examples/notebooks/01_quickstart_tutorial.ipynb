{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkTrainer Quick Start Tutorial\n",
    "\n",
    "This notebook provides a hands-on introduction to SparkTrainer, walking you through the basics of:\n",
    "\n",
    "1. Connecting to the SparkTrainer API\n",
    "2. Listing available models and datasets\n",
    "3. Submitting a simple training job\n",
    "4. Monitoring job progress\n",
    "5. Retrieving trained models\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- SparkTrainer instance running (via Docker Compose)\n",
    "- Python 3.8+\n",
    "- `requests` library installed\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install requests pandas matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Any\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# SparkTrainer API configuration\n",
    "SPARKTRAINER_URL = \"http://localhost:5000\"\n",
    "API_BASE = f\"{SPARKTRAINER_URL}/api\"\n",
    "\n",
    "print(f\"SparkTrainer API: {API_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Health Check\n",
    "\n",
    "Let's verify that SparkTrainer is running and healthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_health():\n",
    "    \"\"\"Check SparkTrainer health status\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{API_BASE}/health\")\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ SparkTrainer is healthy and running!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Health check returned status: {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to SparkTrainer. Is it running?\")\n",
    "        print(\"Run: docker-compose up -d\")\n",
    "        return False\n",
    "\n",
    "check_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Information\n",
    "\n",
    "Let's check available GPU resources and system capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_info():\n",
    "    \"\"\"Get system information including GPU details\"\"\"\n",
    "    response = requests.get(f\"{API_BASE}/system/info\")\n",
    "    return response.json()\n",
    "\n",
    "system_info = get_system_info()\n",
    "\n",
    "print(\"System Resources:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"GPUs: {system_info.get('gpu', {}).get('count', 'N/A')}\")\n",
    "print(f\"GPU Models: {system_info.get('gpu', {}).get('models', [])}\")\n",
    "print(f\"Total Memory: {system_info.get('memory', {}).get('total_gb', 'N/A')} GB\")\n",
    "print(f\"Available Memory: {system_info.get('memory', {}).get('available_gb', 'N/A')} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Browse Available Models\n",
    "\n",
    "Let's see what base models are available for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_base_models():\n",
    "    \"\"\"List all available base models\"\"\"\n",
    "    response = requests.get(f\"{API_BASE}/base-models\")\n",
    "    return response.json()\n",
    "\n",
    "models = list_base_models()\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "models_df = pd.DataFrame(models)\n",
    "print(f\"\\nFound {len(models_df)} available models\\n\")\n",
    "print(models_df[['name', 'family', 'modality', 'parameters', 'trainable']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. List Available Recipes\n",
    "\n",
    "Training recipes define how to train models (LoRA, QLoRA, full fine-tuning, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_recipes():\n",
    "    \"\"\"List all available training recipes\"\"\"\n",
    "    response = requests.get(f\"{API_BASE}/recipes\")\n",
    "    return response.json()\n",
    "\n",
    "recipes = list_recipes()\n",
    "\n",
    "print(\"\\nAvailable Training Recipes:\")\n",
    "print(\"=\" * 50)\n",
    "for recipe in recipes:\n",
    "    print(f\"üìù {recipe['name']}: {recipe.get('description', 'No description')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. List Datasets\n",
    "\n",
    "Check what datasets are already loaded and available for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_datasets():\n",
    "    \"\"\"List all available datasets\"\"\"\n",
    "    response = requests.get(f\"{API_BASE}/datasets\")\n",
    "    return response.json()\n",
    "\n",
    "datasets = list_datasets()\n",
    "\n",
    "if datasets:\n",
    "    datasets_df = pd.DataFrame(datasets)\n",
    "    print(f\"\\nFound {len(datasets_df)} datasets\\n\")\n",
    "    print(datasets_df[['name', 'type', 'size', 'created_at']].head())\n",
    "else:\n",
    "    print(\"No datasets found. Upload a dataset using the UI or API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Training Job\n",
    "\n",
    "Now let's submit a simple LoRA training job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job(config: Dict[str, Any]):\n",
    "    \"\"\"Create a new training job\"\"\"\n",
    "    response = requests.post(f\"{API_BASE}/jobs\", json=config)\n",
    "    return response.json()\n",
    "\n",
    "# Example job configuration\n",
    "job_config = {\n",
    "    \"name\": \"my-first-lora-training\",\n",
    "    \"recipe\": \"lora_qlora\",\n",
    "    \"base_model\": \"meta-llama/Llama-2-7b-hf\",  # Adjust to available model\n",
    "    \"dataset\": \"my-dataset\",  # Replace with your dataset name\n",
    "    \"hyperparameters\": {\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 4,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.05\n",
    "    },\n",
    "    \"resources\": {\n",
    "        \"gpu_count\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Submit the job\n",
    "try:\n",
    "    job = create_job(job_config)\n",
    "    print(f\"‚úÖ Job created successfully!\")\n",
    "    print(f\"Job ID: {job['id']}\")\n",
    "    print(f\"Status: {job['status']}\")\n",
    "    \n",
    "    # Store job ID for monitoring\n",
    "    JOB_ID = job['id']\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating job: {e}\")\n",
    "    JOB_ID = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitor Job Progress\n",
    "\n",
    "Let's monitor the training job in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(job_id: str):\n",
    "    \"\"\"Get current job status\"\"\"\n",
    "    response = requests.get(f\"{API_BASE}/jobs/{job_id}\")\n",
    "    return response.json()\n",
    "\n",
    "def monitor_job(job_id: str, check_interval: int = 5, max_checks: int = 100):\n",
    "    \"\"\"Monitor job until completion or timeout\"\"\"\n",
    "    pbar = tqdm(total=100, desc=\"Training Progress\")\n",
    "    \n",
    "    for _ in range(max_checks):\n",
    "        status = get_job_status(job_id)\n",
    "        \n",
    "        current_status = status.get('status', 'unknown')\n",
    "        progress = status.get('progress', 0)\n",
    "        \n",
    "        pbar.n = progress\n",
    "        pbar.set_description(f\"Status: {current_status}\")\n",
    "        pbar.refresh()\n",
    "        \n",
    "        if current_status in ['completed', 'failed', 'cancelled']:\n",
    "            pbar.close()\n",
    "            return status\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "    \n",
    "    pbar.close()\n",
    "    return status\n",
    "\n",
    "if JOB_ID:\n",
    "    print(f\"Monitoring job {JOB_ID}...\\n\")\n",
    "    final_status = monitor_job(JOB_ID)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Final Status: {final_status['status']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "else:\n",
    "    print(\"No job to monitor. Create a job first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Training Metrics\n",
    "\n",
    "Let's visualize the training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_metrics(job_id: str):\n",
    "    \"\"\"Get job training metrics\"\"\"\n",
    "    response = requests.get(f\"{API_BASE}/jobs/{job_id}/metrics\")\n",
    "    return response.json()\n",
    "\n",
    "if JOB_ID:\n",
    "    metrics = get_job_metrics(JOB_ID)\n",
    "    \n",
    "    # Plot training loss\n",
    "    if 'train_loss' in metrics:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: Training Loss\n",
    "        axes[0].plot(metrics['train_loss'], label='Training Loss', color='blue')\n",
    "        axes[0].set_xlabel('Step')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training Loss Over Time')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Learning Rate\n",
    "        if 'learning_rate' in metrics:\n",
    "            axes[1].plot(metrics['learning_rate'], label='Learning Rate', color='green')\n",
    "            axes[1].set_xlabel('Step')\n",
    "            axes[1].set_ylabel('Learning Rate')\n",
    "            axes[1].set_title('Learning Rate Schedule')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No training metrics available yet.\")\n",
    "else:\n",
    "    print(\"No job to retrieve metrics from.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. List All Jobs\n",
    "\n",
    "View all jobs in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_jobs(status_filter: str = None):\n",
    "    \"\"\"List all jobs with optional status filter\"\"\"\n",
    "    params = {}\n",
    "    if status_filter:\n",
    "        params['status'] = status_filter\n",
    "    \n",
    "    response = requests.get(f\"{API_BASE}/jobs\", params=params)\n",
    "    return response.json()\n",
    "\n",
    "all_jobs = list_jobs()\n",
    "\n",
    "if all_jobs:\n",
    "    jobs_df = pd.DataFrame(all_jobs)\n",
    "    print(f\"\\nTotal Jobs: {len(jobs_df)}\\n\")\n",
    "    print(jobs_df[['id', 'name', 'status', 'created_at']].head(10))\n",
    "    \n",
    "    # Status distribution\n",
    "    status_counts = jobs_df['status'].value_counts()\n",
    "    print(\"\\nJob Status Distribution:\")\n",
    "    print(status_counts)\n",
    "else:\n",
    "    print(\"No jobs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Access Trained Models\n",
    "\n",
    "Once training is complete, you can access the trained model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_adapters(model_id: str):\n",
    "    \"\"\"Get all adapters for a model\"\"\"\n",
    "    response = requests.get(f\"{API_BASE}/models/{model_id}/adapters\")\n",
    "    return response.json()\n",
    "\n",
    "# List available models with adapters\n",
    "models = list_base_models()\n",
    "\n",
    "print(\"\\nModels with Trained Adapters:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model in models[:5]:  # Show first 5\n",
    "    adapters = get_model_adapters(model['id'])\n",
    "    if adapters:\n",
    "        print(f\"\\nüì¶ {model['name']}\")\n",
    "        print(f\"   Adapters: {len(adapters)}\")\n",
    "        for adapter in adapters[:3]:  # Show first 3 adapters\n",
    "            print(f\"   - {adapter['name']} (created: {adapter['created_at']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations! You've completed the SparkTrainer quick start tutorial. Here's what you can explore next:\n",
    "\n",
    "1. **Custom LoRA Recipe Tutorial** (`02_custom_lora_recipe.ipynb`) - Create custom training recipes\n",
    "2. **Multimodal Training** (`03_multimodal_training.ipynb`) - Train vision-language models\n",
    "3. **Advanced Optimization** (`04_advanced_optimization.ipynb`) - Hyperparameter tuning with Optuna\n",
    "4. **Production Deployment** (`05_model_deployment.ipynb`) - Deploy models with vLLM/TGI\n",
    "\n",
    "### Resources\n",
    "\n",
    "- üìö [Full Documentation](https://github.com/def1ant1/SparkTrainer)\n",
    "- üîß [API Reference](http://localhost:5000/api/docs)\n",
    "- üí¨ [GitHub Discussions](https://github.com/def1ant1/SparkTrainer/discussions)\n",
    "- üêõ [Report Issues](https://github.com/def1ant1/SparkTrainer/issues)\n",
    "\n",
    "Happy training! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
