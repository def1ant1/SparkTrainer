# SparkTrainer Model Templates Registry
# This file defines pre-configured model templates for quick training setup

templates:
  # ===================================================================
  # AUTOMATIC SPEECH RECOGNITION (ASR)
  # ===================================================================
  whisper-finetune:
    name: "Whisper Fine-Tune"
    description: "Fine-tune OpenAI Whisper for automatic speech recognition (ASR)"
    category: "asr"
    icon: "mic"
    tags: ["audio", "speech", "transcription", "openai"]
    model:
      base_model: "openai/whisper-base"
      pretrained: true
      architecture: "whisper"
      input_modalities: ["audio"]
      output_type: "text"
    training:
      framework: "huggingface"
      precision: "fp16"
      batch_size: 8
      gradient_accumulation_steps: 2
      learning_rate: 1.0e-5
      num_epochs: 10
      optimizer: "adamw"
      scheduler: "cosine"
      warmup_steps: 500
    data:
      format: "audio_transcription"
      audio_sample_rate: 16000
      max_audio_length: 30
      preprocessing:
        - normalize_audio
        - resample
        - extract_features
    metrics:
      - "wer"  # Word Error Rate
      - "cer"  # Character Error Rate
    resources:
      min_gpu_memory: "16GB"
      recommended_gpu: "A100, H100, or RTX 4090"

  # ===================================================================
  # IMAGE CAPTIONING
  # ===================================================================
  blip2-caption:
    name: "BLIP-2 Image Captioning"
    description: "Fine-tune BLIP-2 for advanced image-to-text captioning"
    category: "vision-language"
    icon: "image"
    tags: ["vision", "language", "captioning", "multimodal"]
    model:
      base_model: "Salesforce/blip2-opt-2.7b"
      pretrained: true
      architecture: "blip2"
      input_modalities: ["image"]
      output_type: "text"
      components:
        vision_encoder: "EVA-CLIP"
        language_model: "OPT-2.7B"
        connector: "Q-Former"
    training:
      framework: "huggingface"
      precision: "bf16"
      batch_size: 4
      gradient_accumulation_steps: 4
      learning_rate: 5.0e-6
      num_epochs: 5
      optimizer: "adamw"
      scheduler: "linear"
      warmup_ratio: 0.03
      freeze_vision_encoder: true
      freeze_language_model: false
    data:
      format: "image_caption"
      image_size: 224
      preprocessing:
        - resize
        - normalize
        - random_crop
        - color_jitter
    metrics:
      - "bleu"
      - "meteor"
      - "rouge"
      - "cider"
    resources:
      min_gpu_memory: "24GB"
      recommended_gpu: "A100 or H100"

  # ===================================================================
  # VISION-LANGUAGE MODELS
  # ===================================================================
  qwen2-vl:
    name: "Qwen2-VL"
    description: "Fine-tune Qwen2-VL for vision-language understanding and generation"
    category: "vision-language"
    icon: "eye"
    tags: ["vision", "language", "multimodal", "qwen", "alibaba"]
    model:
      base_model: "Qwen/Qwen2-VL-7B-Instruct"
      pretrained: true
      architecture: "qwen2_vl"
      input_modalities: ["image", "text"]
      output_type: "text"
      components:
        vision_encoder: "ViT-Large"
        language_model: "Qwen2-7B"
        fusion: "cross-attention"
    training:
      framework: "huggingface"
      precision: "bf16"
      batch_size: 2
      gradient_accumulation_steps: 8
      learning_rate: 2.0e-5
      num_epochs: 3
      optimizer: "adamw"
      scheduler: "cosine"
      warmup_ratio: 0.05
      use_lora: true
      lora_config:
        r: 16
        lora_alpha: 32
        target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
        lora_dropout: 0.05
    data:
      format: "vision_language_instruction"
      image_size: 448
      max_sequence_length: 2048
      preprocessing:
        - resize
        - normalize
        - pad_sequences
    metrics:
      - "accuracy"
      - "bleu"
      - "exact_match"
    resources:
      min_gpu_memory: "40GB"
      recommended_gpu: "A100 or H100"

  # ===================================================================
  # VIDEO GENERATION & DIFFUSION
  # ===================================================================
  svd-video:
    name: "Stable Video Diffusion (SVD)"
    description: "Fine-tune SVD for text-to-video or image-to-video generation"
    category: "diffusion"
    icon: "video"
    tags: ["video", "diffusion", "generation", "stability-ai"]
    model:
      base_model: "stabilityai/stable-video-diffusion-img2vid-xt"
      pretrained: true
      architecture: "stable_video_diffusion"
      input_modalities: ["image", "text"]
      output_type: "video"
      components:
        unet: "UNet3DConditionModel"
        vae: "AutoencoderKL"
        scheduler: "EulerDiscreteScheduler"
    training:
      framework: "diffusers"
      precision: "fp16"
      batch_size: 1
      gradient_accumulation_steps: 16
      learning_rate: 1.0e-4
      num_epochs: 50
      optimizer: "adam"
      scheduler: "constant"
      mixed_precision: true
      enable_xformers: true
      gradient_checkpointing: true
    data:
      format: "video_frames"
      num_frames: 14
      frame_rate: 7
      resolution: [576, 1024]
      preprocessing:
        - extract_frames
        - resize
        - normalize
        - temporal_augmentation
    metrics:
      - "fvd"  # Frechet Video Distance
      - "is"   # Inception Score
      - "lpips"
    resources:
      min_gpu_memory: "80GB"
      recommended_gpu: "H100 or A100-80GB"

  # ===================================================================
  # LARGE LANGUAGE MODELS
  # ===================================================================
  llama3-instruction:
    name: "Llama 3 Instruction Tuning"
    description: "Fine-tune Llama 3 for instruction following and chat"
    category: "language"
    icon: "message-square"
    tags: ["llm", "instruction", "chat", "meta"]
    model:
      base_model: "meta-llama/Meta-Llama-3-8B-Instruct"
      pretrained: true
      architecture: "llama3"
      input_modalities: ["text"]
      output_type: "text"
    training:
      framework: "huggingface"
      precision: "bf16"
      batch_size: 2
      gradient_accumulation_steps: 8
      learning_rate: 2.0e-5
      num_epochs: 3
      optimizer: "adamw"
      scheduler: "cosine"
      warmup_ratio: 0.03
      use_lora: true
      lora_config:
        r: 16
        lora_alpha: 32
        target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        lora_dropout: 0.05
      packing: true
      max_seq_length: 2048
    data:
      format: "instruction"
      prompt_template: "llama3"
      max_sequence_length: 2048
      preprocessing:
        - tokenize
        - pack_sequences
    metrics:
      - "perplexity"
      - "accuracy"
      - "loss"
    resources:
      min_gpu_memory: "24GB"
      recommended_gpu: "A100 or H100"

  # ===================================================================
  # POLYMORPHNET-X MULTIMODAL MODEL (Apotheon.ai)
  # ===================================================================
  polymorphnetx-multimodal:
    name: "PolymorphNet-X Multimodal"
    description: "Apotheon.ai's large-scale multimodal model with MoE architecture for text, image, video, and audio"
    category: "multimodal"
    icon: "layers"
    tags: ["multimodal", "vision", "language", "audio", "video", "moe", "apotheon"]
    model:
      base_model: "custom"
      pretrained: false
      architecture: "polymorphnetx"
      input_modalities: ["text", "image", "video", "audio"]
      output_type: "text"
      components:
        core_model:
          type: "polymorphnetx"
          d_model: 12288
          n_layers: 48
          n_heads: 64
          vocab_size: 131072
          max_seq_len: 8192
          dropout: 0.1
        moe:
          n_experts: 64
          top_k: 4
          expert_ffn_dim: 65536
          capacity_factor: 1.25
        adapters:
          image_adapter:
            type: "conv2d_patch"
            patch_size: 16
            in_channels: 3
          audio_adapter:
            type: "stft_conv1d"
            n_fft: 512
            hop_length: 256
        special_tokens:
          PAD: 0
          UNK: 1
          BOS: 2
          EOS: 3
          AGENT: 10
          MEM: 11
          PLUGIN: 12
          VERIFY: 13
          PRECISION: 14
          NODE: 15
          POLICY: 16
          IMG: 17
          IMG_END: 18
          AUD: 19
          AUD_END: 20
        tokenizer:
          type: "sentencepiece"
          model_prefix: "configs/spm"
          vocab_size: 50000
      objectives:
        - name: "causal_language_modeling"
          weight: 1.0
          loss: "cross_entropy"
        - name: "multimodal_understanding"
          weight: 0.8
          loss: "cross_entropy"
        - name: "load_balance"
          weight: 0.01
          loss: "moe_auxiliary"
    training:
      framework: "pytorch"
      precision: "fp16"
      batch_size: 2
      gradient_accumulation_steps: 16
      learning_rate: 3.0e-4
      num_epochs: 20
      optimizer: "adamw"
      weight_decay: 0.1
      scheduler: "cosine"
      warmup_steps: 2000
      total_steps: 100000
      max_grad_norm: 1.0
      # Advanced training features
      gradient_checkpointing: true
      mixed_precision: true
      use_ema: true
      ema_decay: 0.999
      distributed:
        strategy: "deepspeed_zero3"
        cpu_offload: true
        offload_optimizer: true
      # Curriculum learning for multimodal
      curriculum:
        enabled: true
        stages:
          - steps: [0, 20000]
            modalities: ["text"]
            description: "Text-only warm-up"
          - steps: [20000, 50000]
            modalities: ["text", "image"]
            description: "Add vision modality"
          - steps: [50000, 100000]
            modalities: ["text", "image", "audio", "video"]
            description: "Full multimodal training"
    data:
      format: "multimodal_manifest"
      manifest_schema: "v1"
      preprocessing:
        video:
          extract_frames: true
          num_frames: 8
          frame_rate: 1
          resolution: [224, 224]
        audio:
          extract: true
          sample_rate: 16000
          max_length: 30
        image:
          resize: [224, 224]
          normalize: true
        text:
          max_length: 8192
          tokenizer: "sentencepiece"
      sources:
        - type: "video_with_audio"
          caption_backend: "blip2"
          transcription_backend: "whisper"
        - type: "image_caption"
          caption_backend: "blip2"
        - type: "audio_transcript"
          transcription_backend: "whisper"
        - type: "text_corpus"
          format: "jsonl"
    metrics:
      general:
        - "loss"
        - "perplexity"
        - "token_accuracy"
      moe:
        - "expert_load_balance"
        - "router_entropy"
      multimodal:
        - "modality_specific_accuracy"
    evaluation:
      benchmarks:
        - "coco_captions"
        - "librispeech"
        - "flickr30k"
        - "msrvtt"
    resources:
      min_gpu_memory: "80GB"
      recommended_gpu: "H100, A100-80GB, or 8x A100-40GB"
      min_cpu_memory: "256GB"
      storage: "1TB for checkpoints and data"

  # ===================================================================
  # APOTHEON MULTIMODAL MODEL (NEW)
  # ===================================================================
  apotheon-multimodal:
    name: "Apotheon MultiModal"
    description: "Unified multimodal model for text, image, video, and audio understanding"
    category: "multimodal"
    icon: "layers"
    tags: ["multimodal", "vision", "language", "audio", "video", "unified"]
    model:
      base_model: "custom"
      pretrained: false
      architecture: "apotheon"
      input_modalities: ["text", "image", "video", "audio"]
      output_type: "text"
      components:
        # Vision Encoder
        vision_encoder:
          type: "clip_vit"
          model: "openai/clip-vit-large-patch14"
          hidden_size: 1024
          freeze: false
        # Audio Encoder
        audio_encoder:
          type: "whisper"
          model: "openai/whisper-base"
          hidden_size: 512
          freeze: false
        # Text Encoder/Decoder
        language_model:
          type: "llama3"
          model: "meta-llama/Meta-Llama-3-8B-Instruct"
          hidden_size: 4096
          freeze: false
        # Fusion Transformer
        fusion:
          type: "cross_attention_transformer"
          num_layers: 6
          num_heads: 16
          hidden_size: 2048
          intermediate_size: 8192
          dropout: 0.1
          attention_dropout: 0.1
        # Projection Layers
        projections:
          vision_projection:
            input_dim: 1024
            output_dim: 2048
          audio_projection:
            input_dim: 512
            output_dim: 2048
          text_projection:
            input_dim: 4096
            output_dim: 2048
      # Multi-Task Objectives
      objectives:
        - name: "image_captioning"
          weight: 1.0
          loss: "cross_entropy"
        - name: "audio_transcription"
          weight: 1.0
          loss: "cross_entropy"
        - name: "contrastive_alignment"
          weight: 0.5
          loss: "clip_loss"
          temperature: 0.07
        - name: "video_understanding"
          weight: 0.8
          loss: "cross_entropy"
    training:
      framework: "pytorch"
      precision: "bf16"
      batch_size: 4
      gradient_accumulation_steps: 8
      learning_rate: 3.0e-5
      num_epochs: 10
      optimizer: "adamw"
      weight_decay: 0.01
      scheduler: "cosine"
      warmup_ratio: 0.05
      max_grad_norm: 1.0
      # Modality-specific settings
      modality_sampling:
        strategy: "balanced"  # Sample modalities evenly
        min_modalities_per_batch: 2
      # Advanced training features
      gradient_checkpointing: true
      mixed_precision: true
      distributed:
        strategy: "deepspeed_zero3"
        cpu_offload: true
      # Curriculum learning
      curriculum:
        enabled: true
        stages:
          - epochs: [0, 2]
            modalities: ["text", "image"]
            description: "Warm-up with text and image"
          - epochs: [2, 5]
            modalities: ["text", "image", "audio"]
            description: "Add audio modality"
          - epochs: [5, 10]
            modalities: ["text", "image", "audio", "video"]
            description: "Full multimodal training"
    data:
      format: "multimodal_manifest"
      manifest_schema: "v1"
      preprocessing:
        # Video preprocessing
        video:
          extract_frames: true
          num_frames: 8
          frame_rate: 1
          resolution: [224, 224]
        # Audio preprocessing
        audio:
          extract: true
          sample_rate: 16000
          max_length: 30
        # Image preprocessing
        image:
          resize: [224, 224]
          normalize: true
        # Text preprocessing
        text:
          max_length: 512
          tokenizer: "llama3"
      # Data sources
      sources:
        - type: "video_with_audio"
          caption_backend: "blip2"
          transcription_backend: "whisper"
        - type: "image_caption"
          caption_backend: "blip2"
        - type: "audio_transcript"
          transcription_backend: "whisper"
      # Augmentation
      augmentation:
        enabled: true
        image_augmentation: ["random_crop", "color_jitter", "horizontal_flip"]
        audio_augmentation: ["time_stretch", "pitch_shift", "add_noise"]
        video_augmentation: ["temporal_crop", "spatial_crop"]
    metrics:
      # Image Captioning Metrics
      captioning:
        - "bleu"
        - "meteor"
        - "cider"
        - "rouge"
      # Audio Transcription Metrics
      transcription:
        - "wer"
        - "cer"
      # Contrastive Alignment Metrics
      alignment:
        - "clip_similarity"
        - "retrieval_recall@1"
        - "retrieval_recall@5"
      # Overall Metrics
      general:
        - "loss"
        - "perplexity"
    evaluation:
      # Evaluation benchmarks
      benchmarks:
        - "coco_captions"
        - "librispeech"
        - "flickr30k"
        - "msrvtt"
      # Cross-modal evaluation
      cross_modal_tasks:
        - "image_to_text_retrieval"
        - "text_to_image_retrieval"
        - "video_qa"
        - "audio_visual_matching"
    resources:
      min_gpu_memory: "80GB"
      recommended_gpu: "H100, A100-80GB, or 2x A100-40GB"
      min_cpu_memory: "128GB"
      storage: "500GB for checkpoints and data"
    dataset_pipeline:
      # Automated dataset creation pipeline
      steps:
        - name: "video_ingestion"
          description: "Ingest raw video files"
        - name: "frame_extraction"
          description: "Extract frames from videos"
          params:
            fps: 1
            resolution: [224, 224]
        - name: "audio_extraction"
          description: "Extract audio from videos"
          params:
            sample_rate: 16000
            format: "wav"
        - name: "captioning"
          description: "Generate captions for frames using BLIP-2"
          backend: "blip2"
        - name: "transcription"
          description: "Transcribe audio using Whisper"
          backend: "whisper"
        - name: "scene_detection"
          description: "Detect scene boundaries"
          backend: "pyscenedetect"
        - name: "manifest_generation"
          description: "Create multimodal manifest"
          output: "datasets/multimodal/manifest.jsonl"
      output_structure:
        manifest: "datasets/multimodal/manifest.jsonl"
        frames_dir: "datasets/multimodal/frames/"
        audio_dir: "datasets/multimodal/audio/"
        metadata_dir: "datasets/multimodal/metadata/"

# ===================================================================
# TEMPLATE CATEGORIES
# ===================================================================
categories:
  asr:
    name: "Automatic Speech Recognition"
    description: "Models for speech-to-text transcription"
    icon: "mic"
  vision-language:
    name: "Vision-Language"
    description: "Models that understand both images and text"
    icon: "image"
  diffusion:
    name: "Diffusion Models"
    description: "Generative models for images and videos"
    icon: "wand"
  language:
    name: "Language Models"
    description: "Text generation and understanding"
    icon: "type"
  multimodal:
    name: "Multimodal"
    description: "Unified models for multiple modalities"
    icon: "layers"

# ===================================================================
# TRAINER REGISTRY (Auto-discovered from trainers/*.py)
# ===================================================================
trainer_autodiscovery:
  enabled: true
  search_paths:
    - "src/spark_trainer/trainers"
  pattern: "*.py"
  decorator: "@trainer_template"
  reload_interval: 300  # seconds
