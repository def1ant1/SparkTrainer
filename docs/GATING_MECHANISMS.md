# Gating Mechanisms in SparkTrainer

## Overview

SparkTrainer now supports advanced gating mechanisms for dynamic capacity and smarter compute allocation. These mechanisms enable more efficient model training by selectively routing tokens, adapting model depth, and supporting multi-modal fusion.

## Supported Gating Types

### 1. Token-level MoE (Mixture of Experts)
**Type**: `moe`

Top-K expert routing with capacity factors and load balancing.

**Features**:
- Switch/Top-K routing: Each token routed to K best experts
- Capacity factor: Prevents expert overload
- Z-loss: Auxiliary loss for routing stability
- Gate temperature: Controls routing sharpness

**Use Cases**:
- Scaling model capacity while keeping inference cost constant
- Training sparse models with billions of parameters
- Domain-specific expert specialization

**Configuration**:
```python
from spark_trainer.recipes.recipe_interface import GatingConfig

gating = GatingConfig(
    type='moe',
    num_experts=8,           # Total number of expert modules
    num_selected=2,          # K experts per token
    capacity_factor=1.25,    # Expert capacity multiplier
    gate_temp=1.0,           # Routing temperature
    z_loss_coef=0.01,        # Auxiliary loss coefficient
    enable_metrics=True      # Track utilization metrics
)
```

**Metrics Tracked**:
- Expert utilization: Tokens routed to each expert
- Capacity overflow: Percentage of tokens exceeding capacity
- Z-loss: Routing stability measure
- Gate entropy: Routing diversity
- Expert load variance: Load balance across experts

---

### 2. MoE-LoRA (Low-Rank Expert Adapters)
**Type**: `moe_lora`

Per-expert low-rank adapters for dramatically reduced VRAM footprint.

**Features**:
- 10-100x less VRAM than full MoE
- Per-expert LoRA parameters selected by router
- Ideal for consumer GPUs
- Maintains MoE expressiveness

**Use Cases**:
- Training large MoE models on limited hardware
- Parameter-efficient expert fine-tuning
- Multi-task learning with shared base models

**Configuration**:
```python
gating = GatingConfig(
    type='moe_lora',
    num_experts=8,
    num_selected=2,
    lora_rank=8,             # LoRA rank (lower = less memory)
    lora_alpha=16.0,         # LoRA scaling factor
    capacity_factor=1.25,
)
```

**Memory Savings Example**:
- Full MoE (8 experts, 2048 dim FFN): ~64MB per layer
- MoE-LoRA (8 experts, rank 8): ~0.5MB per layer
- **Reduction**: 128x smaller!

---

### 3. Routerless MoE (DeepSeek-style)
**Type**: `routerless`

Simpler MoE without explicit routing - all experts process all tokens with soft mixing.

**Features**:
- No routing collapse issues
- Simpler training dynamics
- Automatic expert specialization
- No capacity constraints

**Use Cases**:
- When routing collapse is problematic
- Simpler training setup
- Exploration of expert specialization patterns

**Configuration**:
```python
gating = GatingConfig(
    type='routerless',
    num_experts=8,
    dropout=0.1,
)
```

**Advantages**:
- ‚úÖ No routing collapse
- ‚úÖ Simpler optimization
- ‚úÖ No capacity overflow
- ‚ö†Ô∏è All experts always active (no compute savings)

---

### 4. Mixture-of-Depths (Dynamic Layer Selection)
**Type**: `mixture_of_depths`

Tokens dynamically choose which layers to execute - easy tokens exit early.

**Features**:
- Early exit for easy tokens
- 2-3x faster inference
- Adaptive computation per token
- Confidence-based gating

**Use Cases**:
- Inference optimization
- Variable-difficulty token streams
- Latency-critical applications

**Configuration**:
```python
gating = GatingConfig(
    type='mixture_of_depths',
    depth_threshold=0.8,     # Confidence threshold for early exit
    min_layers=1,            # Minimum layers all tokens execute
)
```

**Performance Example**:
- Standard model: 12 layers √ó 1000 tokens = 12,000 layer executions
- Mixture-of-Depths (50% exit at layer 6): ~9,000 layer executions
- **Speedup**: 1.33x with maintained accuracy

**Metrics Tracked**:
- Average depth: Mean layers executed per token
- Exit rate: Percentage of tokens exiting early
- Confidence statistics: Distribution of confidence scores

---

### 5. FiLM Gating (Multi-modal Fusion)
**Type**: `film_gates`

Feature-wise Linear Modulation for adaptive multi-modal fusion.

**Features**:
- Modality-conditioned feature scaling
- Adaptive fusion of text, image, audio, video
- Per-modality affine transformations

**Use Cases**:
- Vision-language models
- Audio-visual learning
- Multi-modal generation

**Configuration**:
```python
gating = GatingConfig(
    type='film_gates',
    num_modalities=3,        # Text=0, Image=1, Audio=2
)
```

**How It Works**:
```
FiLM: y = Œ≥(modality) ‚äô x + Œ≤(modality)

Where:
- x: Input features
- Œ≥: Learned scale parameters (per modality)
- Œ≤: Learned shift parameters (per modality)
- ‚äô: Element-wise multiplication
```

**Metrics Tracked**:
- Modality distribution: Percentage of tokens per modality
- Gamma statistics: Scale parameter magnitudes
- Beta statistics: Shift parameter magnitudes

---

### 6. Span Routing
**Type**: `span_routing`

Routes contiguous token spans together for efficiency.

**Features**:
- Reduced routing overhead
- Shared KV cache across spans
- Better for paragraph/document structure
- Temporal coherence in sequences

**Use Cases**:
- Long-context processing
- Document-level tasks
- Video/audio with temporal structure

**Configuration**:
```python
gating = GatingConfig(
    type='span_routing',
    num_experts=8,
    span_size=32,            # Tokens per span
    span_overlap=8,          # Overlap between spans
)
```

**Efficiency Gains**:
- Standard MoE: N token-level routing operations
- Span routing: N/span_size span-level routing operations
- **Reduction**: 32x fewer routing decisions (with span_size=32)

---

## Integration Guide

### 1. Configuration in Builder UI

Navigate to the Job Wizard and expand the "üöÄ Gating Mechanisms" section:

1. **Enable Gating**: Check the enable checkbox
2. **Select Type**: Choose from dropdown (MoE, MoE-LoRA, etc.)
3. **Configure Parameters**: Adjust based on gating type
4. **Enable Metrics**: Track expert utilization and routing stats

### 2. Recipe Configuration (Python)

```python
from spark_trainer.recipes.recipe_interface import ModelConfig, GatingConfig

# Create gating config
gating = GatingConfig(
    type='moe_lora',
    num_experts=8,
    num_selected=2,
    lora_rank=8,
    enable_metrics=True
)

# Add to model config
model_config = ModelConfig(
    architecture='transformer',
    hidden_size=768,
    num_layers=12,
    gating=gating  # Add gating configuration
)
```

### 3. Using Gating Modules in Custom Models

```python
from spark_trainer.models.gating import (
    create_gating_module,
    MoELoRA,
    MixtureOfDepths,
    FiLMGating,
)

# Option 1: Factory function
gating_config = GatingConfig(type='moe_lora', num_experts=8)
gating_layer = create_gating_module(
    gating_config,
    d_model=768,
    d_ff=3072
)

# Option 2: Direct instantiation
moe_lora = MoELoRA(
    d_model=768,
    d_ff=3072,
    num_experts=8,
    num_selected=2,
    lora_rank=8,
)

# Forward pass
output, metrics = moe_lora(hidden_states)
print(f"Capacity overflow: {metrics.capacity_overflow:.2f}%")
```

### 4. Logging Gating Metrics During Training

```python
import requests

def log_gating_metrics(job_id, step, epoch, metrics):
    """Log gating metrics to SparkTrainer backend."""
    payload = {
        'type': 'moe_lora',
        'step': step,
        'epoch': epoch,
        'expert_utilization': metrics.expert_utilization.tolist(),
        'capacity_overflow': metrics.capacity_overflow,
        'z_loss': metrics.z_loss,
        'gate_entropy': metrics.gate_entropy,
        'expert_load_variance': metrics.expert_load_variance,
    }

    response = requests.post(
        f'http://localhost:5000/api/jobs/{job_id}/gating/metrics',
        json=payload
    )
    return response.json()

# Example training loop
for epoch in range(num_epochs):
    for step, batch in enumerate(dataloader):
        # Forward pass with gating
        output, gating_metrics = model.forward_with_gating(batch)

        # Log metrics every 100 steps
        if step % 100 == 0:
            log_gating_metrics(job_id, step, epoch, gating_metrics)
```

---

## Monitoring and Visualization

### Viewing Gating Metrics

1. **Navigate to Experiments Page**
2. **Select Experiment** with gating-enabled job
3. **Click "üöÄ Gating" Tab**
4. **View Metrics**:
   - Overview: Key metrics (capacity overflow, z-loss, etc.)
   - Heatmap: Expert utilization over time
   - Statistics: Per-expert load balance

### Expert Utilization Heatmap

The heatmap visualization shows:
- **Y-axis**: Expert IDs (0 to N-1)
- **X-axis**: Training steps
- **Color**: Utilization intensity (tokens routed to expert)
- **Load Balance Score**: Overall balance across experts (0-100%)

### Interpreting Metrics

#### Good Indicators:
- ‚úÖ **Capacity Overflow < 5%**: Good load balance
- ‚úÖ **Z-loss < 0.05**: Stable routing
- ‚úÖ **Load Balance Score > 80%**: Well-distributed load
- ‚úÖ **Gate Entropy > 2.0**: Diverse routing decisions

#### Warning Signs:
- ‚ö†Ô∏è **Capacity Overflow > 10%**: Increase capacity_factor
- ‚ö†Ô∏è **Z-loss > 0.1**: Decrease gate_temp or increase z_loss_coef
- ‚ö†Ô∏è **Load Balance Score < 60%**: Expert collapse - adjust capacity/temperature
- ‚ö†Ô∏è **Gate Entropy < 1.0**: All tokens going to few experts - routing collapse

---

## API Reference

### Backend Endpoints

#### Get Gating Metrics
```http
GET /api/jobs/{job_id}/gating/metrics
```

**Response**:
```json
{
  "enabled": true,
  "type": "moe_lora",
  "metrics": {
    "type": "moe_lora",
    "summary": {
      "expert_utilization": [120, 115, 118, 122, 119, 121, 117, 120],
      "capacity_overflow": 3.2,
      "z_loss": 0.015,
      "gate_entropy": 2.78,
      "step": 1000,
      "epoch": 2
    }
  },
  "last_updated": "2024-01-15T10:30:00Z"
}
```

#### Update Gating Metrics
```http
POST /api/jobs/{job_id}/gating/metrics
```

**Request Body**:
```json
{
  "type": "moe_lora",
  "expert_utilization": [120, 115, 118, 122, 119, 121, 117, 120],
  "capacity_overflow": 3.2,
  "z_loss": 0.015,
  "gate_entropy": 2.78,
  "expert_load_variance": 0.025,
  "step": 1000,
  "epoch": 2
}
```

#### Get Expert Utilization Heatmap
```http
GET /api/jobs/{job_id}/gating/expert-utilization
```

**Response**:
```json
{
  "enabled": true,
  "type": "moe_lora",
  "steps": [0, 100, 200, 300, ...],
  "expert_utilization": [
    [100, 105, 98, 102, ...],   // Step 0
    [120, 115, 118, 122, ...],  // Step 100
    ...
  ],
  "num_experts": 8,
  "expert_stats": [
    {
      "expert_id": 0,
      "mean_utilization": 118.5,
      "std_utilization": 12.3,
      "min_utilization": 95,
      "max_utilization": 145
    },
    ...
  ],
  "summary": {
    "total_steps": 1000,
    "load_balance_score": 0.87
  }
}
```

---

## Best Practices

### 1. Choosing the Right Gating Type

| Scenario | Recommended Type | Reason |
|----------|-----------------|---------|
| Large-scale training, unlimited VRAM | `moe` | Maximum capacity scaling |
| Consumer GPUs, VRAM constraints | `moe_lora` | 10-100x memory savings |
| Routing collapse issues | `routerless` | Simpler, more stable training |
| Inference optimization | `mixture_of_depths` | 2-3x faster inference |
| Multi-modal models | `film_gates` | Adaptive modality fusion |
| Long contexts | `span_routing` | Reduced routing overhead |

### 2. Hyperparameter Tuning

**Capacity Factor**:
- Start: 1.25
- Increase if overflow > 10%
- Decrease if overflow < 1% (to improve efficiency)

**Gate Temperature**:
- Start: 1.0
- Increase if z-loss is high (> 0.1)
- Decrease if routing too uniform (entropy < 2.0)

**Z-loss Coefficient**:
- Start: 0.01
- Increase if routing unstable
- Decrease if z-loss dominates total loss

**LoRA Rank** (MoE-LoRA):
- Start: 8
- Increase if capacity insufficient
- Decrease for maximum memory savings

### 3. Monitoring During Training

**Every 100 steps**:
- Check capacity overflow
- Monitor z-loss
- Track expert utilization

**Every epoch**:
- Review heatmap for expert collapse
- Analyze load balance score
- Check for dead experts (zero utilization)

**Action Items**:
- Overflow > 10%: Increase capacity_factor by 0.1
- Z-loss > 0.1: Increase z_loss_coef or gate_temp
- Load balance < 60%: Adjust capacity_factor or add load balancing loss
- Dead experts: Reinitialize or reduce num_experts

---

## Performance Benchmarks

### MoE vs MoE-LoRA Memory Usage

| Configuration | Parameters | Memory (FP16) | Memory Savings |
|---------------|-----------|---------------|----------------|
| Dense FFN | 2M | 4 MB | Baseline |
| MoE (8 experts) | 16M | 32 MB | 8x increase |
| MoE-LoRA (rank 8) | 256K | 0.5 MB | 64x savings vs MoE |

### Mixture-of-Depths Speedup

| Model Size | Layers | Exit Rate | Speedup |
|------------|--------|-----------|---------|
| Base | 12 | 50% @ layer 6 | 1.33x |
| Large | 24 | 60% @ layer 12 | 1.67x |
| XL | 48 | 70% @ layer 24 | 2.14x |

### Span Routing Overhead Reduction

| Sequence Length | Token Routing | Span Routing (32) | Reduction |
|-----------------|---------------|-------------------|-----------|
| 512 | 512 ops | 16 ops | 32x |
| 2048 | 2048 ops | 64 ops | 32x |
| 8192 | 8192 ops | 256 ops | 32x |

---

## Troubleshooting

### Problem: High Capacity Overflow

**Symptoms**: Capacity overflow > 10%

**Solutions**:
1. Increase `capacity_factor` from 1.25 to 1.5 or 2.0
2. Increase `num_experts` to distribute load
3. Increase `num_selected` (K) to allow more routing options

### Problem: Routing Collapse

**Symptoms**:
- All tokens routed to 1-2 experts
- Low gate entropy (< 1.5)
- High load variance

**Solutions**:
1. Increase `gate_temp` to smooth routing
2. Add load balancing auxiliary loss
3. Try `routerless` MoE instead
4. Reduce learning rate for gating parameters

### Problem: High Z-loss

**Symptoms**: Z-loss > 0.1, training instability

**Solutions**:
1. Increase `z_loss_coef` to penalize large logits
2. Increase `gate_temp` to soften routing
3. Add gradient clipping for router parameters
4. Reduce router learning rate

### Problem: OOM (Out of Memory)

**Symptoms**: CUDA out of memory errors

**Solutions**:
1. Use `moe_lora` instead of `moe`
2. Reduce `lora_rank` (try 4 or 8)
3. Reduce `num_experts`
4. Enable gradient checkpointing
5. Reduce batch size

---

## Examples

### Example 1: Training a MoE-LoRA Model for Text Classification

```python
from spark_trainer.recipes.recipe_interface import (
    ModelConfig, GatingConfig, TrainingConfig, DataConfig
)
from spark_trainer.recipes import get_recipe

# Configure gating
gating = GatingConfig(
    type='moe_lora',
    num_experts=8,
    num_selected=2,
    lora_rank=8,
    lora_alpha=16.0,
    enable_metrics=True
)

# Configure model
model_config = ModelConfig(
    architecture='bert',
    pretrained='bert-base-uncased',
    num_classes=10,
    gating=gating
)

# Configure training
training_config = TrainingConfig(
    learning_rate=2e-5,
    epochs=3,
    batch_size=16,
    mixed_precision='fp16'
)

# Configure data
data_config = DataConfig(
    dataset_path='./data/classification',
    batch_size=16
)

# Run training
recipe = get_recipe('bert_classification')(output_dir='./output')
output = recipe.run(data_config, model_config, training_config)
```

### Example 2: Multi-modal Model with FiLM Gating

```python
gating = GatingConfig(
    type='film_gates',
    num_modalities=3,  # Text, Image, Audio
    dropout=0.1
)

model_config = ModelConfig(
    architecture='multimodal_transformer',
    hidden_size=768,
    num_layers=12,
    gating=gating
)

# Forward pass with modality IDs
# modality_ids: 0=text, 1=image, 2=audio
hidden_states = model.embed(inputs)  # [batch, seq, 768]
modality_ids = torch.tensor([0, 0, 0, 1, 1, 2, ...])  # [batch, seq]

film_layer = model.film_gates[0]
output, metrics = film_layer(hidden_states, modality_ids)

print(f"Modality distribution: {metrics['modality_distribution']}")
# Output: [0.5, 0.3, 0.2] => 50% text, 30% image, 20% audio
```

### Example 3: Inference Optimization with Mixture-of-Depths

```python
gating = GatingConfig(
    type='mixture_of_depths',
    depth_threshold=0.8,  # High confidence => early exit
    min_layers=2,         # At least 2 layers for all tokens
)

model_config = ModelConfig(
    architecture='gpt',
    num_layers=24,
    gating=gating
)

# During inference
hidden = model.embed(input_ids)
for layer_idx, layer in enumerate(model.layers):
    # Check which tokens should continue
    continue_mask, depth_metrics = model.depth_gate.should_continue(
        hidden, layer_idx
    )

    print(f"Layer {layer_idx}: {depth_metrics['exit_rate']:.1%} tokens exited")

    if not continue_mask.any():
        print(f"All tokens exited at layer {layer_idx}")
        break

    # Only process continuing tokens
    hidden, _ = model.depth_gate.forward_with_depth_gating(
        hidden, layer_idx, layer
    )
```

---

## Citation

If you use gating mechanisms in SparkTrainer for your research, please cite:

```bibtex
@software{sparktrainer_gating,
  title={SparkTrainer: Gating Mechanisms for Dynamic Capacity},
  author={SparkTrainer Contributors},
  year={2024},
  url={https://github.com/def1ant1/SparkTrainer}
}
```

---

## Additional Resources

- **Paper**: [Switch Transformers](https://arxiv.org/abs/2101.03961) - Original MoE work
- **Paper**: [DeepSeek-MoE](https://arxiv.org/abs/2401.06066) - Routerless MoE
- **Paper**: [Mixture-of-Depths](https://arxiv.org/abs/2404.02258) - Dynamic depth
- **Paper**: [FiLM](https://arxiv.org/abs/1709.07871) - Feature-wise modulation
- **Tutorial**: [MoE Best Practices](https://docs.spark-trainer.com/moe-guide)
- **Discord**: [SparkTrainer Community](https://discord.gg/sparktrainer)

---

## Contributing

We welcome contributions to improve gating mechanisms! Areas for contribution:
- New gating types (e.g., Expert Choice, Soft MoE)
- Better routing algorithms
- Optimization techniques
- Bug fixes and performance improvements

See [CONTRIBUTING.md](../CONTRIBUTING.md) for guidelines.
